# -*- coding: utf-8 -*-
import json
from datetime import date, timedelta

import pysnooper  # debug ç”¨çš„åŒ…
import scrapy
from scrapy.loader import ItemLoader
import time

from ..items import News
from ..items import NewsContent



class XinlangspiderSpider(scrapy.Spider):
    name = 'news'
    # çˆ¬å–çš„åŸŸåï¼Œä¸ä¼šè¶…å‡ºè¿™ä¸ªé¡¶çº§åŸŸå
    allowed_domains = ['sina.com']  # å¯ä»¥è®¾ç½®æˆä¸è¿‡æ»¤å—ã€‚
    start_urls = [
    ]

    count = 1
    # {}å ä½ç¬¦ï¼Œç”¨äºå­—ç¬¦ä¸²æ›¿æ¢ï¼Œå°†è·å–åˆ°çš„/text/page/1æ ¼å¼å†…å®¹æ›¿æ¢æˆå®Œæ•´url  è¿™ä¸ªæ˜¯æ–°æµªæ–°é—»çš„ã€‚æ»šåŠ¨æ–°é—»çš„é¡µé¢
    host_url = 'https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2509&k=&num=50&page={}'

    def start_requests(self):
        for num in range(1,60):  # è¿™å„¿æ˜¯çœ‹çˆ¬å–å¤šå°‘é¡µçš„ã€‚50*60 =3000 å·²ç»å¤Ÿå¤šçš„äº†ã€‚æ–°æµªçš„
            print(self.host_url.format(num))
            self.start_urls.append(self.host_url.format(num))
        for url in self.start_urls:  # ç¬¬ä¸€å±‚ç®—æ˜¯å¹¿åº¦ä¼˜å…ˆçˆ¬å–å¯¹å§ã€‚
            yield scrapy.Request(url, dont_filter=False)
        #     # è¿™é‡Œé‡å†™çˆ¬è™«å…¥å£æ–¹æ³•ï¼Œå°†dont_filterè®¾ç½®ä¸ºfalse
        #     # æ˜¯ä¸ºäº†è®©èµ·å§‹urlæ”¾å…¥srcapy.Requestè¯·æ±‚urlæ± ä¸­ï¼Œå¯¹èµ·å§‹urlä¹Ÿåšå»é‡å¤„ç†
        #     # å¦åˆ™ä¼šçˆ¬å–åˆ°ä¸¤æ¬¡ https://www.qiushibaike.com/text/ï¼Œä¸€æ¬¡æ˜¯èµ·å§‹url
        #     # ä¸€æ¬¡æ˜¯åˆ†é¡µæ•°æ®é‡Œæ£€ç´¢åˆ°çš„ç¬¬ä¸€é¡µ


    def parse(self, response):  # æ¯ä¸€é¡µçš„éƒ½åœ¨è¿™å„¿äº†ã€‚
        # itemloader
        allDic = json.loads(response.body)
        # print(allDic)
        print(type(allDic))
        for one in allDic['result']['data']:
            # print(one['url'])
            # print(one['title'])
            timeStamp = one['intime']
            timeArray = time.localtime(int(timeStamp))
            newsDatetemp = time.strftime("%Y-%m-%d %H:%M:%S", timeArray)
            newsDate = newsDatetemp.split(" ")[0]   # è¿™ä¸ªæ˜¯æ—¥æœŸçš„å­—ç¬¦ä¸²
            # print(newsDate)
            # print(one['intime'])

            # è¿™å„¿åšä¸€ä¸ªæµ‹è¯•ï¼Œç”¨æ¥çœ‹æ˜¯ä¸æ˜¯åœ¨åŒä¸€ä¸ªåœ°æ–¹çš„ã€‚ä¸è¦å½“å¤©çš„ï¼Œè¿™å„¿æ³¨æ„
            lastDay = (date.today() + timedelta(days=-1)).strftime("%Y-%m-%d")  # è·å–æ˜¨å¤©æ—¥æœŸ
            lastDayEnd = (date.today() + timedelta(days=-2)).strftime("%Y-%m-%d")  # è·å–å‰å¤©æ—¥æœŸ

            # ç¬¬äºŒå¤©çˆ¬å–ä¼šå‡ºç°ä»Šå¤©çš„ï¼Œè¿˜æœ‰æ˜¨å¤©çš„ï¼Œè¿˜æœ‰å‰å¤©çš„ã€‚
            if newsDate == lastDay:
                # æ·»åŠ è¿›item
                itemloader = ItemLoader(item=News(), response=response, )
                itemloader.add_value('url', one['url'])
                itemloader.add_value('title', one['title'])
                itemloader.add_value('timestamp', one['intime'])
                itemloader.add_value('newsDate', newsDate)
                resultItem = itemloader.load_item()
                yield scrapy.Request(url=resultItem['url'][0], callback=self.newsContent, dont_filter=True,
                                     meta={"lastItem": resultItem})

            if newsDate == lastDayEnd: # è¿™ä¸ªæ˜¯å‰å¤©çš„
                break  # è¿™å„¿å°±æ˜¯è·³å‡ºå¾ªç¯äº†

            else:    # æ˜¯ä»Šå¤©çš„æƒ…å†µå°±ä¸ç”¨ç®¡äº†
                print("è¿™æ¡æ˜¯ä»Šå¤©çš„ï¼Œè·³è¿‡çˆ¬å–---ã€‹å› ä¸ºæˆ‘ä»¬è¦æå–å®Œæ•´çš„æ˜¨å¤©çš„æ–°é—»ã€‚ğŸ˜")
                print()
        # print("è¿™å„¿è¿™ä¸ªå°±æ˜¯è·‘å®Œäº†ï¼")

    # è¿™è¾¹æ˜¯è§£æè¯¦æƒ…é¡µçš„éƒ¨åˆ†ã€‚
    @pysnooper.snoop()  #è¿™æ ·å°±å¯ä»¥debugäº†
    def newsContent(self,response):
        print()
        print()
        lastItem = response.meta["lastItem"]
        print(lastItem['url'][0])
        print(lastItem['title'][0])
        print(lastItem['newsDate'][0])
        # print(response.body)
        contentlist = []
        print("å…¨æ–‡åœ¨è¿™å„¿äº†")
        # print(response.xpath("//div[@class='article']").xpath('string(.)').extract_first())
        for allp in response.xpath("//div[@class='article']"):   # //div[@class='article'] ï¼Œè¦å–è¿™ä¸‹é¢çš„æ‰€æœ‰çš„æ–‡æœ¬å¯¹å§
            print(allp.xpath("p"))
            for p in allp.xpath("p"):
                # print(p.xpath("text()").extract_first())
                contentlist.append(p.xpath("string(.)").extract_first().strip())  # æ¢ç”¨è¿™ç§åå‘¢ï¼Œä¼šä¸ä¼šå°±ä¸ä¼šå†å‘ç”Ÿé‚£ç§äº‹æƒ…äº†ã€‚
            print()
        print()
        print(contentlist)
        # time.sleep(60)

        # print(contentlist)    # todo æœ‰æ—¶å€™æ˜¯Noneçš„å›å» ç ”ç©¶ä¸€ä¸‹è¿™éƒ¨åˆ†çš„éƒ¨åˆ†
        print(len(contentlist))
        tempContent = ""
        if len(contentlist)== 0 :
            tempContent=""
        else:
            # è¿™å„¿å¯èƒ½å›åˆå¹¶å‡ºé”™çš„ã€‚åˆå¹¶å‡ºé”™å°±å†è¯•ä¸€è¯•å’¯ã€‚åº”è¯¥æ²¡ä»€ä¹ˆå¤§é—®é¢˜çš„ã€‚
            tempContent = "".join(contentlist)     # todoæ˜¯è¿™å„¿çš„é—®é¢˜æŠŠï¼Œä¹Ÿå°±æ˜¯è¯´å¯èƒ½contentlisté‡Œé¢å¹¶ä¸æ˜¯çº¯æ–‡æœ¬çš„ã€‚


        print("æ£€æŸ¥ç¬¬å‡ ä¸ª{}".format(self.count))
        self.count=self.count+1
        print(tempContent)
        newsloader = ItemLoader(item=NewsContent(), response=response)
        newsloader.add_value('Pcontent',tempContent)
        newsloader.add_value('title',lastItem['title'][0])
        newsloader.add_value('url',lastItem['url'][0])
        newsloader.add_value("newsDate",lastItem['newsDate'][0])

        print(lastItem['newsDate'][0])
        # time.sleep(15)

        yield newsloader.load_item()
        # time.sleep(30)
        pass
